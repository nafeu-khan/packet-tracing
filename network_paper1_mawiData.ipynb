{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ganQFZ-fx3Bidf6xDlbT7PeKXDnwb_7O",
      "authorship_tag": "ABX9TyOOGSRnhR02j6acnpa9yle5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nafeu-khan/packet-tracing/blob/main/network_paper1_mawiData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bugttp0jxMog",
        "outputId": "240c118b-b6ac-40c3-8273-eef9c62c6bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312251400.html\n",
            "Fetching URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312251400.html (Attempt 1/5)\n",
            "Data saved to extracted_data/2023/12/25\n",
            "Data saved to 202312251400-total.csv\n",
            "Data saved to extracted_data/2023/12/25\n",
            "Data saved to 202312251400-ip.csv\n",
            "Data saved to extracted_data/2023/12/25\n",
            "Data saved to 202312251400-ip6.csv\n",
            "Processing URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312261400.html\n",
            "Fetching URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312261400.html (Attempt 1/5)\n",
            "Data saved to extracted_data/2023/12/26\n",
            "Data saved to 202312261400-total.csv\n",
            "Data saved to extracted_data/2023/12/26\n",
            "Data saved to 202312261400-ip.csv\n",
            "Data saved to extracted_data/2023/12/26\n",
            "Data saved to 202312261400-ip6.csv\n",
            "Processing URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312271400.html\n",
            "Fetching URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312271400.html (Attempt 1/5)\n",
            "Data saved to extracted_data/2023/12/27\n",
            "Data saved to 202312271400-total.csv\n",
            "Data saved to extracted_data/2023/12/27\n",
            "Data saved to 202312271400-ip.csv\n",
            "Data saved to extracted_data/2023/12/27\n",
            "Data saved to 202312271400-ip6.csv\n",
            "Processing URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312281400.html\n",
            "Fetching URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312281400.html (Attempt 1/5)\n",
            "Data saved to extracted_data/2023/12/28\n",
            "Data saved to 202312281400-total.csv\n",
            "Data saved to extracted_data/2023/12/28\n",
            "Data saved to 202312281400-ip.csv\n",
            "Data saved to extracted_data/2023/12/28\n",
            "Data saved to 202312281400-ip6.csv\n",
            "Processing URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312291400.html\n",
            "Fetching URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312291400.html (Attempt 1/5)\n",
            "Data saved to extracted_data/2023/12/29\n",
            "Data saved to 202312291400-total.csv\n",
            "Data saved to extracted_data/2023/12/29\n",
            "Data saved to 202312291400-ip.csv\n",
            "Data saved to extracted_data/2023/12/29\n",
            "Data saved to 202312291400-ip6.csv\n",
            "Processing URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312301400.html\n",
            "Fetching URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312301400.html (Attempt 1/5)\n",
            "Data saved to extracted_data/2023/12/30\n",
            "Data saved to 202312301400-total.csv\n",
            "Data saved to extracted_data/2023/12/30\n",
            "Data saved to 202312301400-ip.csv\n",
            "Data saved to extracted_data/2023/12/30\n",
            "Data saved to 202312301400-ip6.csv\n",
            "Processing URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312311400.html\n",
            "Fetching URL: https://mawi.wide.ad.jp/mawi/samplepoint-F/2023/202312311400.html (Attempt 1/5)\n",
            "Data saved to extracted_data/2023/12/31\n",
            "Data saved to 202312311400-total.csv\n",
            "Data saved to extracted_data/2023/12/31\n",
            "Data saved to 202312311400-ip.csv\n",
            "Data saved to extracted_data/2023/12/31\n",
            "Data saved to 202312311400-ip6.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "import csv\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import os\n",
        "# Generate URLs for fetching the data\n",
        "def generate_urls(start_year, end_year):\n",
        "    current_date = datetime(start_year, 1, 1, 14, 0)  # December 25th, 2:00 PM\n",
        "\n",
        "    while current_date.year <= end_year:\n",
        "        year = current_date.year\n",
        "        formatted_date = current_date.strftime(\"%Y%m%d%H%M\")\n",
        "        url = f\"https://mawi.wide.ad.jp/mawi/samplepoint-F/{year}/{formatted_date}.html\"\n",
        "        yield url\n",
        "\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "# Parse the hierarchical data from the <pre> tag\n",
        "def parse_pre_tag(pre_text):\n",
        "    data = defaultdict(lambda: defaultdict(dict))  # Nested structure: data['ip']['tcp']['http']\n",
        "    current_category = None\n",
        "    current_subcategory = None\n",
        "\n",
        "    lines = pre_text.strip().split(\"\\n\")\n",
        "    for line in lines[2:]:\n",
        "\n",
        "        # Match top-level categories like 'ip', 'ip6', etc.\n",
        "        match_category = re.match(\n",
        "            r'^\\s*(\\w+)\\s+(\\d+)\\s+\\(\\s*(\\d+\\.\\d+)%\\)\\s+(\\d+)\\s+\\(\\s*(\\d+\\.\\d+)%\\)\\s+(\\d+\\.\\d+)',\n",
        "            line\n",
        "        )\n",
        "        if match_category and not line.startswith(\"  \"):\n",
        "            current_category = match_category.group(1)  # Category name (e.g., ip, ip6)\n",
        "            current_subcategory = None\n",
        "\n",
        "            # Store the data for the category\n",
        "            data[current_category]['total'] = {\n",
        "                'packets': match_category.group(2),\n",
        "                'packets_percentage': match_category.group(3),\n",
        "                'bytes': match_category.group(4),\n",
        "                'bytes_percentage': match_category.group(5),\n",
        "                'bytes_per_pkt': match_category.group(6)\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        # Match subcategories like 'tcp', 'udp', etc. (indented by 2 spaces)\n",
        "        match_subcategory = re.match(\n",
        "            r'^\\s{2}(\\S+)\\s+(\\d+)\\s+\\(\\s*(\\d+\\.\\d+)%\\)\\s+(\\d+)\\s+\\(\\s*(\\d+\\.\\d+)%\\)\\s+(\\d+\\.\\d+)',\n",
        "            line\n",
        "        )\n",
        "        if match_subcategory:\n",
        "            current_subcategory = match_subcategory.group(1).strip()\n",
        "            data[current_category][current_subcategory]['total'] = {\n",
        "                'packets': match_subcategory.group(2),\n",
        "                'packets_percentage': match_subcategory.group(3),\n",
        "                'bytes': match_subcategory.group(4),\n",
        "                'bytes_percentage': match_subcategory.group(5),\n",
        "                'bytes_per_pkt': match_subcategory.group(6)\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        # Match third-level data like 'http', 'https', etc. (indented by 3 spaces)\n",
        "        match_data = re.match(\n",
        "            r'^\\s{3}(\\w+)\\s+(\\d+)\\s+\\(\\s*(\\d+\\.\\d+)%\\)\\s+(\\d+)\\s+\\(\\s*(\\d+\\.\\d+)%\\)\\s+(\\d+\\.\\d+)',\n",
        "            line\n",
        "        )\n",
        "        if match_data and current_category and current_subcategory:\n",
        "            protocol = match_data.group(1)\n",
        "            data[current_category][current_subcategory][protocol] = {\n",
        "                'packets': match_data.group(2),\n",
        "                'packets_percentage': match_data.group(3),\n",
        "                'bytes': match_data.group(4),\n",
        "                'bytes_percentage': match_data.group(5),\n",
        "                'bytes_per_pkt': match_data.group(6)\n",
        "            }\n",
        "\n",
        "    return data\n",
        "\n",
        "def extract_traffic_info(soup):\n",
        "    info = {}\n",
        "\n",
        "    # Look for traffic trace information\n",
        "    traffic_info = soup.find('h3', string='Traffic Trace Info')\n",
        "    if traffic_info:\n",
        "        text = traffic_info.find_next('br').find_parent().text\n",
        "\n",
        "        # Extract FileSize\n",
        "        match_filesize = re.search(r'FileSize:\\s+(\\d+\\.\\d+MB)', text)\n",
        "        if match_filesize:\n",
        "            info['filesize'] = match_filesize.group(1)\n",
        "\n",
        "    return info\n",
        "\n",
        "def flatten_data(data, timestamp, filesize):\n",
        "    flattened = defaultdict(list)\n",
        "\n",
        "    for category, subcategories in data.items():\n",
        "        for subcategory, protocols in subcategories.items():\n",
        "            for protocol, metrics in protocols.items():\n",
        "                if isinstance(metrics, dict):\n",
        "                    flatten_row = {\n",
        "                        \"Timestamp\": timestamp,\n",
        "                        \"FileSize\": filesize,\n",
        "                        \"FileSize(MB)\": float(re.search(r'(\\d+\\.\\d+)([A-Z]+)', filesize).group(1)),\n",
        "                        \"Protocol\": f\"{subcategory}-{protocol}\" if subcategory != 'total' else protocol,\n",
        "                        **metrics\n",
        "                    }\n",
        "                    flattened[category].append(flatten_row)\n",
        "                else:\n",
        "                    # Handling invalid case properly\n",
        "                    flatten_row = {\n",
        "                        \"Timestamp\": timestamp,\n",
        "                        \"FileSize\": filesize,\n",
        "                        \"FileSize(MB)\": float(re.search(r'(\\d+\\.\\d+)([A-Z]+)', filesize).group(1)),\n",
        "                        \"Protocol\": f\"{category}-{protocol}\",\n",
        "                        \"packets\": protocols.get('packets', ''),\n",
        "                        \"packets_percentage\": protocols.get('packets_percentage', ''),\n",
        "                        \"bytes\": protocols.get('bytes', ''),\n",
        "                        \"bytes_percentage\": protocols.get('bytes_percentage', ''),\n",
        "                        \"bytes_per_pkt\": protocols.get('bytes_per_pkt', ''),\n",
        "                    }\n",
        "                    flattened[category].append(flatten_row)  # Append to the correct list\n",
        "                    break\n",
        "    return flattened\n",
        "\n",
        "\n",
        "# Extract data from the URL with retry mechanism\n",
        "def extract_data(url, timestamp, max_retries=5, backoff_factor=2):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"Fetching URL: {url} (Attempt {attempt + 1}/{max_retries})\")\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Extract traffic trace info\n",
        "            traffic_info = extract_traffic_info(soup)\n",
        "            filesize = traffic_info.get('filesize', 'Unknown')\n",
        "\n",
        "            pre_tag = soup.find('pre')\n",
        "\n",
        "            if not pre_tag:\n",
        "                print(f\"No <pre> tag found in {url}\")\n",
        "                return {}\n",
        "\n",
        "            data = parse_pre_tag(pre_tag.text)\n",
        "            return flatten_data(data, timestamp, filesize)\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Timeout error for {url}. Retrying...\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"Max retries reached. Skipping {url}.\")\n",
        "                return {}\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"Max retries reached. Skipping {url}.\")\n",
        "                return {}\n",
        "\n",
        "        # Exponential backoff\n",
        "        time.sleep(backoff_factor ** attempt)\n",
        "\n",
        "    return {}\n",
        "\n",
        "# Create the directory structure if it doesn't exist\n",
        "def create_directory_structure(filename):\n",
        "    year = filename[:4]\n",
        "    month = filename[4:6]\n",
        "    day = filename[6:8]\n",
        "    directory = f\"extracted_data/{year}/{month}/{day}\"\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    return directory\n",
        "\n",
        "# Save data to CSV\n",
        "def save_to_csv(filename, data, headers):\n",
        "    directory = create_directory_structure(filename)\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=headers)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "    print(f\"Data saved to {directory}/{filename}\")\n",
        "\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    start_year = 2016\n",
        "    end_year = 2024\n",
        "\n",
        "    headers = [\n",
        "        \"Timestamp\", \"FileSize\",\"FileSize(MB)\", \"Protocol\", \"packets\", \"packets_percentage\", \"bytes\", \"bytes_percentage\", \"bytes_per_pkt\"\n",
        "    ]\n",
        "\n",
        "    for url in generate_urls(start_year, end_year):\n",
        "        print(f\"Processing URL: {url}\")\n",
        "        timestamp = url.split('/')[-1].split('.')[0]\n",
        "        extracted_data = extract_data(url, timestamp)\n",
        "        if extracted_data:\n",
        "            for category, data in extracted_data.items():\n",
        "                filename = f\"{timestamp}-{category}.csv\"\n",
        "                save_to_csv(filename, data, headers)\n",
        "                # print(f\"Data saved to {filename}\")\n",
        "        else:\n",
        "            print(f\"No data extracted for {url}\")\n"
      ]
    }
  ]
}